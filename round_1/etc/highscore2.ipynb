{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<konlpy.tag._twitter.Twitter at 0x102f54630>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Twitter\n",
    "import jpype\n",
    "Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB , GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "pos_tagger = Twitter()\n",
    "# read train data\n",
    "\n",
    "with open('./data/train_data.json') as fp:\n",
    "    json_str = fp.read()\n",
    "    json_data = json.loads(json_str)\n",
    "    \n",
    "# convert to dataframe\n",
    "train_df = pd.DataFrame(json_data)\n",
    "\n",
    "# train data preprocessing\n",
    "train_df['rating_cat'] = train_df['rating'].apply(lambda x: \n",
    "                    'NEG' if 1<= x <=3 \n",
    "                     else \n",
    "                      ('NEU' if 4<=x<=7 \n",
    "                     else 'POS'))\n",
    "\n",
    "\n",
    "\n",
    "input_file = './data/test.input'\n",
    "\n",
    "with open(input_file) as fp:\n",
    "    test_in = fp.read()\n",
    "\n",
    "test_in = test_in.splitlines()\n",
    "\n",
    "\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "\n",
    "def tokenize_pos(doc):\n",
    "    return ['/'.join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "vectorizer =  TfidfVectorizer(tokenizer=tokenize_pos, ngram_range=(1,3), use_idf=False, smooth_idf=False)\n",
    "\n",
    "\n",
    "y = train_df.rating_cat\n",
    "X = vectorizer.fit_transform(train_df.review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble = model = SGDClassifier(loss='modified_huber',alpha=2.9e-6).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=30, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "# model1 = LinearSVC()\n",
    "model = LogisticRegression(C=30)\n",
    "# model3 = LinearSVC(C=0.5)\n",
    "# ensemble = VotingClassifier(estimators=[('svc1', model1), ('logit', model2), ('svc2', model3)], \n",
    "#                             voting='soft', weights=[2, 1, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = vectorizer.get_feature_names()\n",
    "\n",
    "input_file = './data/grading.input'\n",
    "\n",
    "with open(input_file) as fp:\n",
    "    test_in = fp.read()\n",
    "\n",
    "test_in = test_in.splitlines()\n",
    "\n",
    "\n",
    "test_vectorizer =  TfidfVectorizer(tokenizer=tokenize_pos, ngram_range=(1,3), vocabulary = feature_list)\n",
    "X_test = test_vectorizer.fit_transform(test_in[:8400])\n",
    "\n",
    "\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'submission.txt'#sys.argv[2] # \n",
    "\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    for _test_pred in test_pred:\n",
    "        fp.write(_test_pred)\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = vectorizer.get_feature_names()\n",
    "\n",
    "input_file = './data/test.input'\n",
    "\n",
    "with open(input_file) as fp:\n",
    "    test_in = fp.read()\n",
    "\n",
    "test_in = test_in.splitlines()\n",
    "\n",
    "\n",
    "test_vectorizer =  TfidfVectorizer(tokenizer=tokenize_pos, ngram_range=(1,3), vocabulary = feature_list)\n",
    "X_test = test_vectorizer.fit_transform(test_in[:8400])\n",
    "\n",
    "\n",
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.62      0.57      0.59      1022\n",
      "        NEU       0.59      0.43      0.50      1825\n",
      "        POS       0.83      0.92      0.87      5553\n",
      "\n",
      "avg / total       0.75      0.77      0.76      8400\n",
      "\n",
      "0.769047619048\n",
      "0.7685\n",
      "0.778\n",
      "0.7385\n",
      "0.7875\n"
     ]
    }
   ],
   "source": [
    "#ngram 13 , 2.9le6\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.63      0.55      0.58      1022\n",
      "        NEU       0.68      0.36      0.47      1825\n",
      "        POS       0.81      0.95      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.75      8400\n",
      "\n",
      "0.774761904762\n",
      "0.7815\n",
      "0.7855\n",
      "0.7415\n",
      "0.787916666667\n"
     ]
    }
   ],
   "source": [
    "#ngram 13 , 2.9le6\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.63      0.56      0.59      1022\n",
      "        NEU       0.59      0.46      0.51      1825\n",
      "        POS       0.84      0.91      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.76      8400\n",
      "\n",
      "0.771785714286\n",
      "0.774\n",
      "0.782\n",
      "0.742\n",
      "0.78625\n"
     ]
    }
   ],
   "source": [
    "#ngram 13 , 1e6\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.57      0.19      0.29      1022\n",
      "        NEU       0.49      0.17      0.25      1825\n",
      "        POS       0.71      0.95      0.81      5553\n",
      "\n",
      "avg / total       0.65      0.69      0.63      8400\n",
      "\n",
      "0.688214285714\n",
      "0.683\n",
      "0.713\n",
      "0.6355\n",
      "0.715833333333\n"
     ]
    }
   ],
   "source": [
    "# test out ngram 1,3 logistic C= 30\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.67      0.00      0.00      1022\n",
      "        NEU       0.60      0.00      0.00      1825\n",
      "        POS       0.66      1.00      0.80      5553\n",
      "\n",
      "avg / total       0.65      0.66      0.53      8400\n",
      "\n",
      "0.661666666667\n",
      "0.6605\n",
      "0.681\n",
      "0.6035\n",
      "0.695\n"
     ]
    }
   ],
   "source": [
    "# test out ngram 1,3 logistic\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.67      0.00      0.00      1022\n",
      "        NEU       0.00      0.00      0.00      1825\n",
      "        POS       0.66      1.00      0.80      5553\n",
      "\n",
      "avg / total       0.52      0.66      0.53      8400\n",
      "\n",
      "0.66130952381\n",
      "0.6605\n",
      "0.6805\n",
      "0.603\n",
      "0.694583333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoo/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# test out ngram 1,3 sgd\n",
    "with open('./data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.77797619  0.77595238  0.7777381   0.77666667  0.7775    ]\n",
      " [ 0.77833333  0.77690476  0.77738095  0.77654762  0.77690476]\n",
      " [ 0.7777381   0.7777381   0.77702381  0.77690476  0.77678571]\n",
      " [ 0.77761905  0.7775      0.7777381   0.77714286  0.77785714]\n",
      " [ 0.7777381   0.7775      0.77833333  0.77702381  0.77797619]\n",
      " [ 0.7772619   0.77833333  0.77642857  0.77678571  0.77642857]\n",
      " [ 0.7772619   0.7772619   0.77678571  0.7772619   0.77702381]\n",
      " [ 0.77738095  0.77809524  0.77785714  0.77654762  0.77797619]\n",
      " [ 0.77738095  0.77785714  0.77761905  0.7775      0.7777381 ]\n",
      " [ 0.77678571  0.77702381  0.77797619  0.7772619   0.77761905]]\n",
      "[ 0.77716667  0.77721429  0.7772381   0.77757143  0.77771429  0.77704762\n",
      "  0.77711905  0.77757143  0.77761905  0.77733333]\n"
     ]
    }
   ],
   "source": [
    "# n_iter 최적\n",
    "\n",
    "n = np.arange(15,25)\n",
    "\n",
    "\n",
    "ite = 5 #  반복\n",
    "score = np.zeros([len(n), ite])\n",
    "\n",
    "\n",
    "for i, val in enumerate(n):\n",
    "    for k in range(ite): \n",
    "        \n",
    "        model = SGDClassifier(alpha=2.9e-6, n_iter=val).fit(X, y)\n",
    "        test_pred = model.predict(X_test)\n",
    "\n",
    "        score[i][k] = accuracy_score(test_out[:8400], test_pred)\n",
    "        \n",
    "print(score)\n",
    "print(score.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SGDClassifier(alpha=2.9e-6, n_iter=19).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.77619048  0.7775      0.77607143  0.77511905  0.77583333]\n",
      " [ 0.77630952  0.77452381  0.775       0.77714286  0.77547619]\n",
      " [ 0.77738095  0.77642857  0.77583333  0.77738095  0.77559524]\n",
      " [ 0.77488095  0.77595238  0.77428571  0.77404762  0.77440476]\n",
      " [ 0.77630952  0.77619048  0.77452381  0.7775      0.7747619 ]\n",
      " [ 0.77714286  0.77666667  0.77559524  0.77630952  0.77535714]\n",
      " [ 0.77595238  0.775       0.77511905  0.775       0.77642857]\n",
      " [ 0.77369048  0.77690476  0.77607143  0.77488095  0.77392857]\n",
      " [ 0.77440476  0.77357143  0.77428571  0.775       0.77595238]\n",
      " [ 0.77547619  0.77392857  0.77488095  0.77464286  0.7752381 ]]\n",
      "[ 0.77614286  0.77569048  0.77652381  0.77471429  0.77585714  0.77621429\n",
      "  0.7755      0.77509524  0.77464286  0.77483333]\n"
     ]
    }
   ],
   "source": [
    "#alpha 최적값 찾기\n",
    "#alpha = np.arange(0.0000039,0.000004, 0.00000001)\n",
    "#alpha = np.arange(0.000001,0.00001,0.000001) ## 3e-6 이 일등\n",
    "alpha = np.arange(2.5e-6,3.5e-6,1e-7) ## 2.9e-6이 일등\n",
    "#alpha = [0.001, 0.0001, 0.00001, 0.000001, 0.]\n",
    "ite = 5 #  반복\n",
    "score = np.zeros([len(alpha), ite])\n",
    "\n",
    "for i, val in enumerate(alpha):\n",
    "    for k in range(ite): \n",
    "        model = SGDClassifier(alpha=val).fit(X, y)\n",
    "        test_pred = model.predict(X_test)\n",
    "\n",
    "        score[i][k] = accuracy_score(test_out[:8400], test_pred)\n",
    "        \n",
    "print(score)\n",
    "print(score.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775833333333\n",
      "0.775714285714\n",
      "0.774642857143\n",
      "0.774285714286\n",
      "0.77619047619\n",
      "0.776666666667\n",
      "0.774880952381\n",
      "0.775\n",
      "0.774761904762\n",
      "0.772976190476\n"
     ]
    }
   ],
   "source": [
    "alpha = np.arange(0.0000039,0.000004, 0.00000001)\n",
    "\n",
    "for i in alpha:\n",
    "    model = SGDClassifier(alpha=i).fit(X, y)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    print(accuracy_score(test_out[:8400], test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775119047619\n",
      "0.776428571429\n",
      "0.774166666667\n",
      "0.775\n",
      "0.775\n",
      "0.775595238095\n",
      "0.775119047619\n",
      "0.775\n",
      "0.77619047619\n",
      "0.775\n"
     ]
    }
   ],
   "source": [
    "alpha = np.arange(0.0000039,0.000004, 0.00000001)\n",
    "a = (0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001)\n",
    "for i in alpha:\n",
    "    model = SGDClassifier(alpha=i).fit(X, y)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    print(accuracy_score(test_out[:8400], test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = (0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001)\n",
    "for i in alpha:\n",
    "    model = SGDClassifier(alpha=i).fit(X, y)\n",
    "    test_pred = model.predict(X_test)\n",
    "\n",
    "    print(accuracy_score(test_out[:8400], test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SGDClassifier(alpha=0.0000039).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.64      0.55      0.59      1022\n",
      "        NEU       0.68      0.38      0.48      1825\n",
      "        POS       0.81      0.95      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.78      0.76      8400\n",
      "\n",
      "0.776547619048\n",
      "0.7815\n",
      "0.7845\n",
      "0.747\n",
      "0.790416666667\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (3.9*e-6) #high score # 최대\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.63      0.56      0.59      1022\n",
      "        NEU       0.66      0.39      0.49      1825\n",
      "        POS       0.81      0.94      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.78      0.76      8400\n",
      "\n",
      "0.775238095238\n",
      "0.7835\n",
      "0.7815\n",
      "0.7475\n",
      "0.78625\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (3.5*e-6)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.63      0.55      0.59      1022\n",
      "        NEU       0.67      0.37      0.48      1825\n",
      "        POS       0.81      0.95      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.75      8400\n",
      "\n",
      "0.774285714286\n",
      "0.783\n",
      "0.783\n",
      "0.743\n",
      "0.785833333333\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (4*e-6)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.64      0.56      0.60      1022\n",
      "        NEU       0.66      0.37      0.48      1825\n",
      "        POS       0.81      0.95      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.78      0.75      8400\n",
      "\n",
      "0.775714285714\n",
      "0.7815\n",
      "0.7865\n",
      "0.7455\n",
      "0.787083333333\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (3*e-6)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.65      0.53      0.58      1022\n",
      "        NEU       0.68      0.34      0.46      1825\n",
      "        POS       0.80      0.96      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.75      8400\n",
      "\n",
      "0.77130952381\n",
      "0.7785\n",
      "0.782\n",
      "0.743\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (7*e-5)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.64      0.55      0.59      1022\n",
      "        NEU       0.69      0.35      0.46      1825\n",
      "        POS       0.80      0.95      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.75      8400\n",
      "\n",
      "0.77369047619\n",
      "0.7805\n",
      "0.7825\n",
      "0.745\n",
      "0.784583333333\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (5*e-5)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.55      0.56      0.56      1022\n",
      "        NEU       0.51      0.45      0.48      1825\n",
      "        POS       0.84      0.87      0.85      5553\n",
      "\n",
      "avg / total       0.73      0.74      0.73      8400\n",
      "\n",
      "0.739642857143\n",
      "0.721\n",
      "0.7575\n",
      "0.713\n",
      "0.7625\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (1.e-6)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.62      0.59      0.60      1022\n",
      "        NEU       0.61      0.43      0.50      1825\n",
      "        POS       0.83      0.92      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.76      8400\n",
      "\n",
      "0.773571428571\n",
      "0.776\n",
      "0.779\n",
      "0.743\n",
      "0.7925\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (1.e-5)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.65      0.53      0.58      1022\n",
      "        NEU       0.70      0.31      0.43      1825\n",
      "        POS       0.79      0.96      0.87      5553\n",
      "\n",
      "avg / total       0.75      0.77      0.74      8400\n",
      "\n",
      "0.765595238095\n",
      "0.77\n",
      "0.7775\n",
      "0.7325\n",
      "0.779583333333\n"
     ]
    }
   ],
   "source": [
    "# test out sgd (1.e-4)\n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "#sgd()\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n",
    "\n",
    "print(accuracy_score(test_out[6400:8400], test_pred[6400:8400]))\n",
    "print(accuracy_score(test_out[4400:6400], test_pred[4400:6400]))\n",
    "print(accuracy_score(test_out[2400:4400], test_pred[2400:4400]))\n",
    "print(accuracy_score(test_out[:2400], test_pred[:2400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.86      0.05      0.10      1022\n",
      "        NEU       0.60      0.00      0.01      1825\n",
      "        POS       0.67      1.00      0.80      5553\n",
      "\n",
      "avg / total       0.68      0.67      0.54      8400\n",
      "\n",
      "0.667976190476\n"
     ]
    }
   ],
   "source": [
    "# test out sgd \n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "\n",
    "#t(koken twit pos) - s\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.70      0.27      0.39      1022\n",
      "        NEU       0.71      0.09      0.16      1825\n",
      "        POS       0.71      0.99      0.82      5553\n",
      "\n",
      "avg / total       0.71      0.71      0.63      8400\n",
      "\n",
      "0.705595238095\n"
     ]
    }
   ],
   "source": [
    "# test out sgd \n",
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()\n",
    "\n",
    "\n",
    "#t(koken twit pos) - s\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 실패\n",
    "from konlpy.tag import Twitter\n",
    "pos_tagger = Twitter()\n",
    "\n",
    "def tokenize_pos(doc):\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "                \n",
    "\n",
    "model = Pipeline([\n",
    "            ('vect', TfidfVectorizer(tokenizer=tokenize_pos, ngram_range=(1,2))), \n",
    "            ('mb',   SVC(kernel='linear')),\n",
    "        ])\n",
    "\n",
    "\n",
    "X = train_df.review\n",
    "y = train_df.rating_cat\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 최고기록 0.7730\n",
    "from konlpy.tag import Twitter\n",
    "pos_tagger = Twitter()\n",
    "\n",
    "def tokenize_pos(doc):\n",
    "    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "                \n",
    "model = Pipeline([\n",
    "            ('vect', TfidfVectorizer(tokenizer=tokenize_pos, ngram_range=(1,2))), \n",
    "            ('mb',   LogisticRegression()),\n",
    "        ])\n",
    "\n",
    "\n",
    "X = train_df.review\n",
    "y = train_df.rating_cat\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 낮음\n",
    "model = Pipeline([\n",
    "            ('vect', TfidfVectorizer()), \n",
    "            ('mb', LogisticRegression(C=10.0)),\n",
    "        ])\n",
    "\n",
    "X = train_df.review\n",
    "y = train_df.rating_cat\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    ['종합', '평점', '점']\n",
       "1    ['원작', '칭송', '이유', '웹툰', '계', '자체', '질적', '저하'...\n",
       "2    ['나름', '감동', '마음', '가슴', '배우', '연기', '김수현', '최고']\n",
       "3    ['걸', '돈', '자신', '후회', '최악', '쓰레기', '영화', '김수현...\n",
       "4               ['초반', '코미디', '후반', '액션', '결론', '코미디']\n",
       "5                       ['김수현', '일', '처리', '처리', '절차']\n",
       "6                                    ['원작', '어디', '팔']\n",
       "7                        ['나름', '장면', '해도', '나름', '볼']\n",
       "8                                ['님', '이영화', '클레멘타인']\n",
       "9                    ['좀', '억지', '감동', '좀', '유치', '함']\n",
       "Name: review_nouns, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review_nouns'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [ ' 종 합 ' ,   ' 평 점 ' ,   ' 점 ' ]\n",
       "1    [ ' 원 작 ' ,   ' 칭 송 ' ,   ' 이 유 ' ,   ' 웹 툰 ' ...\n",
       "2    [ ' 나 름 ' ,   ' 감 동 ' ,   ' 마 음 ' ,   ' 가 슴 ' ...\n",
       "3    [ ' 걸 ' ,   ' 돈 ' ,   ' 자 신 ' ,   ' 후 회 ' ,   ...\n",
       "4    [ ' 초 반 ' ,   ' 코 미 디 ' ,   ' 후 반 ' ,   ' 액 션 ...\n",
       "5    [ ' 김 수 현 ' ,   ' 일 ' ,   ' 처 리 ' ,   ' 처 리 ' ...\n",
       "6                    [ ' 원 작 ' ,   ' 어 디 ' ,   ' 팔 ' ]\n",
       "7    [ ' 나 름 ' ,   ' 장 면 ' ,   ' 해 도 ' ,   ' 나 름 ' ...\n",
       "8            [ ' 님 ' ,   ' 이 영 화 ' ,   ' 클 레 멘 타 인 ' ]\n",
       "9    [ ' 좀 ' ,   ' 억 지 ' ,   ' 감 동 ' ,   ' 좀 ' ,   ...\n",
       "Name: review_nouns, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['review_nouns'][:10].astype(str).apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['review_nouns'] = train_df['review'].apply(twitter.nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-24f97c1ce3be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df.to_csv('train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['저', '사람']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.nouns('안녕하세요 저는 사람입니다. 사랑합니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8400 features per sample; expecting 1862890",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bc4235b1dd7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 317\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 8400 features per sample; expecting 1862890"
     ]
    }
   ],
   "source": [
    "#input_file = '/elice/data/grading.input'#sys.argv[1] #'/elice/data/test.input' #\n",
    "input_file = '/elice/data/test.input' \n",
    "output_file = 'submission.txt'#sys.argv[2] # \n",
    "\n",
    "\n",
    "\n",
    "with open(input_file) as fp:\n",
    "    test_in = fp.read()\n",
    "\n",
    "test_in = test_in.splitlines()\n",
    "test_pred = model.predict(test_in[:8400])\n",
    "\n",
    "\n",
    "with open(output_file, 'w') as fp:\n",
    "    for _test_pred in test_pred:\n",
    "        fp.write(_test_pred)\n",
    "        fp.write('\\n')\n",
    "        \n",
    "#t(koken twit pos) - s\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "pos_tagger = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/elice/data/test.output') as fp:\n",
    "    test_out = fp.read()\n",
    "    test_out = test_out.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.64      0.53      0.58      1022\n",
      "        NEU       0.62      0.42      0.50      1825\n",
      "        POS       0.82      0.93      0.87      5553\n",
      "\n",
      "avg / total       0.76      0.77      0.76      8400\n",
      "\n",
      "0.773095238095\n"
     ]
    }
   ],
   "source": [
    "#t-l(koken twit pos)\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.60      0.48      0.53      1022\n",
      "        NEU       0.55      0.41      0.47      1825\n",
      "        POS       0.81      0.91      0.85      5553\n",
      "\n",
      "avg / total       0.73      0.75      0.73      8400\n",
      "\n",
      "0.74630952381\n"
     ]
    }
   ],
   "source": [
    "#t-l(c=10))\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.61      0.58      0.60      1022\n",
      "        NEU       0.58      0.44      0.50      1825\n",
      "        POS       0.84      0.91      0.87      5553\n",
      "\n",
      "avg / total       0.75      0.77      0.76      8400\n",
      "\n",
      "0.768333333333\n"
     ]
    }
   ],
   "source": [
    "#t-linear svc\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        NEG       0.57      0.57      0.57      1022\n",
      "        NEU       0.52      0.44      0.48      1825\n",
      "        POS       0.84      0.88      0.86      5553\n",
      "\n",
      "avg / total       0.74      0.75      0.74      8400\n",
      "\n",
      "0.74619047619\n"
     ]
    }
   ],
   "source": [
    "#t-linear svc (5)\n",
    "print(classification_report(test_out[:8400],test_pred))\n",
    "print(accuracy_score(test_out[:8400], test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4391"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
